library(ggplot2)
detach("package:ggplot2", unload = TRUE)
install.packages("essquise")
install.packages("essquire")
install.packages(c("backports", "callr", "jsonlite", "knitr", "MASS", "mgcv", "nlme", "openssl", "processx", "stringi", "survival", "tinytex", "withr", "xfun"))
install.packages("essquire")
install.packages("essquise")
install.packages("tidyverse")
install.packages("essquise")
install.packages("esquisse")
################################################
## This file contains commands used for ##
## the spam problem ##
## ##
## Sierra Widman ##
################################################
library(rpart)
################################################
## Read in spam data ##
################################################
set.seed(1)
spam.data <- read.table("C:/Users/Kumupins/Desktop/School/UCSD/Predictive Analytics/spambase.txt", sep =",",na.strings = "NA")
################################################
## Check dimensions of data input
dim(spam.data)
################################################
## label columns with variable names ##
names(spam.data)<-c("wfmake", "wfaddress", "wfall", "wf3d", "wfour", "wfover", "wfremove", "wfinternet", "wforder", "wfmail", "wfreceive", "wfwill", "wfpeople", "wfreport", "wfaddresses", "wffree", "wfbusiness", "wfemail", "wfyou", "wfcredit", "wfyour", "wffont", "wf000", "wfmoney", "wfhp", "wfhpl", "wfgeorge", "wf650", "wflab", "wflabs", "wftelnet", "wf857", "wfdata", "wf415", "wf85", "wftechnology", "wf1999", "wfparts", "wfpm", "wfdirect", "wfcs", "wfmeeting", "wforiginal", "wfproject", "wfre", "wfedu", "wftable", "wfconference", "cfsc", "cfpar", "cfbrack", "cfexc", "cfdollar", "cfpound", "crlaverage", "crllongest", "crltotal", "spam")
#####################################################
# Specify that the variable "spam" is
# categorical and name the categories:
#See decision tree R code file
#specifically from that file example
spam.data$spam <- factor(spam.data$spam, levels=0:1, labels=c("No", "Prog"))
################################################
## This file contains commands used for ##
## the spam problem ##
## ##
## Sierra Widman ##
################################################
library(rpart)
################################################
## Read in spam data ##
################################################
set.seed(1)
spam.data <- read.table("/Users/elizabethfabio/Downloads/spambase/spambase.data ", sep =",",na.strings = "NA")
################################################
## Check dimensions of data input
dim(spam.data)
################################################
## label columns with variable names ##
names(spam.data)<-c("wfmake", "wfaddress", "wfall", "wf3d", "wfour", "wfover", "wfremove", "wfinternet", "wforder", "wfmail", "wfreceive", "wfwill", "wfpeople", "wfreport", "wfaddresses", "wffree", "wfbusiness", "wfemail", "wfyou", "wfcredit", "wfyour", "wffont", "wf000", "wfmoney", "wfhp", "wfhpl", "wfgeorge", "wf650", "wflab", "wflabs", "wftelnet", "wf857", "wfdata", "wf415", "wf85", "wftechnology", "wf1999", "wfparts", "wfpm", "wfdirect", "wfcs", "wfmeeting", "wforiginal", "wfproject", "wfre", "wfedu", "wftable", "wfconference", "cfsc", "cfpar", "cfbrack", "cfexc", "cfdollar", "cfpound", "crlaverage", "crllongest", "crltotal", "spam")
#####################################################
# Specify that the variable "spam" is
# categorical and name the categories:
#See decision tree R code file
#specifically from that file example
spam.data$spam <- factor(spam.data$spam, levels=0:1, labels=c("No", "Prog"))
spam.data <- read.table("/Users/elizabethfabio/Downloads/spambase/spambase.data ", sep =",",na.strings = "NA")
spam.data <- read.table("/Users/elizabethfabio/Downloads/spambase/spambase.data/ ", sep =",",na.strings = "NA")
spam.data <- read.table("/Users/elizabethfabio/Downloads/spambase/spambase.data", sep =",",na.strings = "NA")
################################################
## Check dimensions of data input
dim(spam.data)
################################################
## label columns with variable names ##
names(spam.data)<-c("wfmake", "wfaddress", "wfall", "wf3d", "wfour", "wfover", "wfremove", "wfinternet", "wforder", "wfmail", "wfreceive", "wfwill", "wfpeople", "wfreport", "wfaddresses", "wffree", "wfbusiness", "wfemail", "wfyou", "wfcredit", "wfyour", "wffont", "wf000", "wfmoney", "wfhp", "wfhpl", "wfgeorge", "wf650", "wflab", "wflabs", "wftelnet", "wf857", "wfdata", "wf415", "wf85", "wftechnology", "wf1999", "wfparts", "wfpm", "wfdirect", "wfcs", "wfmeeting", "wforiginal", "wfproject", "wfre", "wfedu", "wftable", "wfconference", "cfsc", "cfpar", "cfbrack", "cfexc", "cfdollar", "cfpound", "crlaverage", "crllongest", "crltotal", "spam")
#####################################################
# Specify that the variable "spam" is
# categorical and name the categories:
#See decision tree R code file
#specifically from that file example
spam.data$spam <- factor(spam.data$spam, levels=0:1, labels=c("No", "Prog"))
################################################
## This file contains commands used for ##
## the spam problem ##
## ##
## Sierra Widman ##
################################################
library(rpart)
################################################
## Read in spam data ##
################################################
set.seed(1)
spam.data <- read.table("/Users/elizabethfabio/Downloads/spambase/spambase.data", sep =",",na.strings = "NA")
################################################
## Check dimensions of data input
dim(spam.data)
################################################
## label columns with variable names ##
names(spam.data)<-c("wfmake", "wfaddress", "wfall", "wf3d", "wfour", "wfover", "wfremove", "wfinternet", "wforder", "wfmail", "wfreceive", "wfwill", "wfpeople", "wfreport", "wfaddresses", "wffree", "wfbusiness", "wfemail", "wfyou", "wfcredit", "wfyour", "wffont", "wf000", "wfmoney", "wfhp", "wfhpl", "wfgeorge", "wf650", "wflab", "wflabs", "wftelnet", "wf857", "wfdata", "wf415", "wf85", "wftechnology", "wf1999", "wfparts", "wfpm", "wfdirect", "wfcs", "wfmeeting", "wforiginal", "wfproject", "wfre", "wfedu", "wftable", "wfconference", "cfsc", "cfpar", "cfbrack", "cfexc", "cfdollar", "cfpound", "crlaverage", "crllongest", "crltotal", "spam")
#####################################################
# Specify that the variable "spam" is
# categorical and name the categories:
#See decision tree R code file
#specifically from that file example
spam.data$spam <- factor(spam.data$spam, levels=0:1, labels=c("No", "Prog"))
spam.data
table <- spam.data
View(table)
names(spam.data)
################################################
## This file contains commands used for ##
## the spam problem ##
## ##
## Sierra Widman ##
################################################
library(rpart)
################################################
## Read in spam data ##
################################################
set.seed(1)
spam.data <- read.table("/Users/elizabethfabio/Downloads/spambase/spambase.data", sep =",",na.strings = "NA")
################################################
## Check dimensions of data input
dim(spam.data)
################################################
## label columns with variable names ##
names(spam.data)<-c("wfmake", "wfaddress", "wfall", "wf3d", "wfour", "wfover", "wfremove", "wfinternet", "wforder", "wfmail", "wfreceive", "wfwill", "wfpeople", "wfreport", "wfaddresses", "wffree", "wfbusiness", "wfemail", "wfyou", "wfcredit", "wfyour", "wffont", "wf000", "wfmoney", "wfhp", "wfhpl", "wfgeorge", "wf650", "wflab", "wflabs", "wftelnet", "wf857", "wfdata", "wf415", "wf85", "wftechnology", "wf1999", "wfparts", "wfpm", "wfdirect", "wfcs", "wfmeeting", "wforiginal", "wfproject", "wfre", "wfedu", "wftable", "wfconference", "cfsc", "cfpar", "cfbrack", "cfexc", "cfdollar", "cfpound", "crlaverage", "crllongest", "crltotal", "spam")
#####################################################
# Specify that the variable "spam" is
# categorical and name the categories:
#See decision tree R code file
#specifically from that file example
spam.data$spam <- factor(spam.data$spam, levels=0:1, labels=c("No", "Prog"))
names(spam.data)
install.packages("plyr")
library(stringr)
library(plyr)
install.packages(c("backports", "broom", "callr", "cli", "clipr", "codetools", "colorspace", "cpp11", "dbplyr", "Deriv", "digest", "doBy", "esquisse", "generics", "KernSmooth", "labeling", "lme4", "lubridate", "nlme", "NLP", "ps", "R6", "readr", "rlang", "rmarkdown", "rstudioapi", "shinyWidgets", "statmod", "survival", "testthat", "tibble", "tinytex", "xfun"))
library(stringr)
library(tm)
library(SnowballC)
install.packages("lda")
library(lda)
install.packages("LDAvis")
library(LDAvis)
install.packages(c("plyr","stringr","tm","SnowballC","lda","LDAvis"))
install.packages(c("plyr", "stringr", "tm", "SnowballC", "lda", "LDAvis"))
library("plyr")
library("stringr")
library("tm")
library("SnowballC")
library("lda")
library("LDAvis")
setwd("/Users/elizabethfabio/Documents/SCHOOL/BDA594-Fabio/Web_Exercises/WebExercise7_TopicModel&Video/TwitterSample.csv")
setwd("/Users/elizabethfabio/Documents/SCHOOL/BDA594-Fabio/Web_Exercises/WebExercise7_TopicModel&Video/TwitterSample.csv")
setwd("/Users/elizabethfabio/Documents/SCHOOL/BDA594-Fabio/Web_Exercises/WebExercise7_TopicModel&Video/")
Twitter<-read.csv("TwitterSample.csv")
View(Twitter)
Tweets_corpus <- tm_map(Tweets_corpus, tolower)
Tweets_corpus <- Corpus(VectorSource(Twitter$TWEET_TEXT))
View(Tweets_corpus)
View(Tweets_corpus)
Tweets_corpus <- tm_map(Tweets_corpus, tolower)
View(Tweets_corpus)
View(Twitter)
> Tweets_corpus[[1]]
Tweets_corpus[[1]]
Tweets_corpus <- tm_map(Tweets_corpus, removePunctuation)
Tweets_corpus <- tm_map(Tweets_corpus, removeNumbers)
Tweets_corpus <- tm_map(Tweets_corpus, function(x) gsub("http[[:alnum:]]*","", x))
Tweets_corpus <- tm_map(Tweets_corpus, function(x) iconv(x, "latin1", "ASCII", sub=""))
Tweets_corpus <- tm_map(Tweets_corpus, removeWords,stopwords("SMART"))
View(Tweets_corpus)
Tweets_corpus[["1"]]
Tweets_corpus <- tm_map(Tweets_corpus, removeWords,c("london", "im","ive", "dont", "didnt"))
Tweets_corpus[["1"]][["content"]]
Tweets_corpus <- tm_map(Tweets_corpus, stripWhitespace)
Tweets_corpus[["1"]][["content"]]
View(Twitter)
View(Twitter)
View(Tweets_corpus)
View(Twitter)
Tweets_corpus[[1]]
Tweets_corpus <- tm_map(Tweets_corpus, PlainTextDocument)
Tweets_corpus <- tm_map(Tweets_corpus, stemDocument)
Tweets_corpus[["1"]][["content"]]
Tweets_corpus[["1"]]
Tweet_Clean<-as.data.frame(unlist(sapply(Tweets_corpus[[1]]$content,'[')), stringsAsFactors=F)
View(Tweet_Clean)
Tweet_Clean <- lapply(Tweet_Clean[,1], function(x) gsub("^ ", "", x)) #multiple spaces Tweet_Clean <- lapply(Tweet_Clean, function(x) gsub("^[[:space:]]+", "", x)) #space at the begining Tweet_Clean <- lapply(Tweet_Clean, function(x) gsub("[[:space:]]+$", "", x)) #space at the end
View(Tweet_Clean)
View(Tweet_Clean)
Twitter$Tweet_Clean<-Tweet_Clean
Twitter[1:10,]
View(Tweet_Clean)
doc.list <- strsplit(unlist(Tweet_Clean), "[[:space:]]+")
View(doc.list)
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
term.table <- term.table[term.table>3]
vocab <- names(term.table)
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
# Compute some statistics related to the data set: D <- length(documents) # number of documents
View(documents)
D <- length(documents) # number of documents
W <- length(vocab) # number of terms in the vocab
doc.length <- sapply(documents, function(x) sum(x[2, ])) # number of tokens per document
N <- sum(doc.length) # total number of tokens in the data
term.frequency <- as.integer(term.table) # frequencies of terms in the corpus
K <- 20
G <- 1000
alpha <- 0.1
eta <- 0.1
t1 <- print(Sys.time())
lda_fit <- lda.collapsed.gibbs.sampler (documents = documents, K = K, vocab = vocab, num.iterations = G, alpha = alpha, eta = eta)
t2 <- print(Sys.time())
t2-t1
View(lda_fit)
top_words<-top.topic.words(lda_fit$topics,20,by.score=TRUE)
View(top_words)
theta <- t(apply(lda_fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(lda_fit$topics) + eta, 2, function(x) x/sum(x)))
Tweet_Topics <- list(phi = phi, theta = theta, doc.length = doc.length, vocab = vocab, term.frequency = term.frequency)
Tweet_Topics_json <- with(Tweet_Topics,createJSON(phi, theta, doc.length, vocab, term.frequency))
serVis(Tweet_Topics_json)
doc_topic <- apply(lda_fit$document_sums, 2, function(x) which(x == max(x))[1])
Twitter$topic<-doc_topic
TwitterDf <- data.frame(lapply(Twitter, as.character), stringsAsFactors=FALSE)
write.csv(TwitterDf,"TwitterWithTopic.csv")
